%%writefile reducer_sum.py

num_words = 0
stopwords = 0
for line in sys.stderr:
    counter = num_words.append(line.split(':')[2].split(',')
    if counter[1].startswith('Totalwords'):
        totalwords += counter[2]
    else:
        stopwords += counter[2]
                               
eprint("reporter:counter:Wiki stats,Totalwords,%d" % len(totalwords))
eprint("reporter:counter:Wiki stats,Stopwords,%d" % len(stopwords))               


%%bash

OUT_DIR="coursera_mr_task2"
NUM_REDUCERS=4
LOGS="stderr_logs.txt"

hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null

# Stub code for your job

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapreduce.job.name="StopWords" \
    -D mapreduce.job.reduces=${NUM_REDUCERS} \
    -files mapper_wiki_parser.py,reducer_sum.py, stop_words_en.txt \
    -mapper "python3 mapper_wiki_parser.py" \
    -reducer "python3 reducer_sum.py" \
    -input /data/wiki/en_articles_part \
    -output ${OUT_DIR} > /dev/null 2> $LOGS
    
cat $LOGS | python ./counter_process.py "Stop words" "Total words"
cat $LOGS >&2